
"""
influence_variance_estimator.py

אומדן שונות אסימפטוטית לפי פונקציית ההשפעה (Influence Function) עבור g-functional
בנקודת-חשיפה (Point exposure) עם מדיניות π(A|L) (סטטית או דינמית).

המודול הזה מיועד לעבודה עם:
- bn שלך (class bn) לצורך דומיינים (bn.domains) ובדיקות בסיסיות.
- נתוני תצפית (DataFrame / CSV) שג'ינרטת מהרשת.

הוא מחשב:
1) רכיבי nuisance מהנתונים: b(a,z)=E[Y|A=a,Z=z], Var(Y|A=a,Z=z), P(Z), P(A,Z), f(A|Z)
2) m(z)=Σ_a b(a,z)π(a|L(z)), ו- χ = E[m(Z)]
3) ערכי פונקציית ההשפעה ψ_i לכל תצפית
4) אומדן השונות האסימפטוטית:  σ^2 = E[ψ^2]  (אמידה ע"י ממוצע אמפירי)

שימי לב:
- זה אומדן "plug-in" מהדאטה (לא חישוב אנליטי דרך bn.conditional/marginal_prob).
- יש טיפול אופציונלי ב-zero counts באמצעות Laplace smoothing (alpha>0).

Author: generated by ChatGPT
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union

import numpy as np
import pandas as pd

# טיפוס למדיניות: policy_fn(a_val, L_assign_dict) -> π(a|L)
PolicyFn = Callable[[Any, Dict[str, Any]], float]


def static_do_policy(a_star: Any) -> PolicyFn:
    """
    מדיניות סטטית: do(A=a_star)
    π(a|L)=1 אם a==a_star אחרת 0.
    """
    def _pi(a_val: Any, L_assign: Dict[str, Any]) -> float:
        return 1.0 if a_val == a_star else 0.0
    return _pi


@dataclass(frozen=True)
class NuisanceEstimates:
    """
    מחזיק את כל האמידות הדרושות לחישוב ψ ולחישוב σ^2.
    """
    # b(a,z) ו-Var(Y|a,z)
    b_map: Dict[Tuple[Any, Tuple[Any, ...]], float]
    var_map: Dict[Tuple[Any, Tuple[Any, ...]], float]

    # P(Z) ו-P(A,Z)
    PZ: Dict[Tuple[Any, ...], float]
    PAZ: Dict[Tuple[Any, Tuple[Any, ...]], float]

    # f(a|z)
    f_map: Dict[Tuple[Any, Tuple[Any, ...]], float]

    # π(a|L(z)) כתלות ב-(a,z)
    pi_map: Dict[Tuple[Any, Tuple[Any, ...]], float]

    # w(a,z)=π/f
    w_map: Dict[Tuple[Any, Tuple[Any, ...]], float]

    # m(z) ו-χ
    m_map: Dict[Tuple[Any, ...], float]
    chi: float


# -------------------------
# Utilities: keys & typing
# -------------------------

def _as_tuple(x: Any, expected_len: int) -> Tuple[Any, ...]:
    """
    Pandas groupby returns scalar for 1-column key, tuple for multi-column.
    Normalize to tuple length expected_len.
    """
    if expected_len == 1:
        return (x,)
    if isinstance(x, tuple):
        return x
    return (x,)  # fallback


def _z_tuple_from_row(row: pd.Series, Z_vars: Sequence[str]) -> Tuple[Any, ...]:
    return tuple(row[z] for z in Z_vars)


def _validate_columns(df: pd.DataFrame, cols: Sequence[str]) -> None:
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing columns in observations: {missing}")


# -----------------------------------------
# Step 1: b(a,z) and Var(Y|a,z) from data
# -----------------------------------------

def estimate_b_and_var_Y_given_AZ(
    df: pd.DataFrame,
    *,
    Y: str,
    A: str,
    Z_vars: Sequence[str],
    value_map: Optional[Mapping[Any, float]] = None,
) -> Tuple[Dict[Tuple[Any, Tuple[Any, ...]], float],
           Dict[Tuple[Any, Tuple[Any, ...]], float]]:
    """
    מאמד מהנתונים:
      b(a,z)   = E[Y | A=a, Z=z]
      var(a,z) = Var(Y | A=a, Z=z)

    מימוש רעיוני: groupby על (A,Z) וחישוב mean ו-var.

    value_map:
      אם Y בינארי {0,1} אפשר להשאיר None.
      אחרת, ספקי מיפוי לערכים מספריים.
    """
    _validate_columns(df, [Y, A, *Z_vars])

    # map Y to numeric
    if value_map is None:
        # ברירת מחדל: ננסה להבין אם Y הוא {0,1}
        uniq = set(df[Y].dropna().unique().tolist())
        if uniq.issubset({0, 1}):
            value_map = {0: 0.0, 1: 1.0}
        else:
            raise ValueError(
                "Y אינו בינארי {0,1}. ספקי value_map שממפה כל ערך אפשרי של Y למספר."
            )

    work = df.copy()
    work["_Ynum"] = work[Y].map(value_map).astype(float)

    group_cols = [A, *Z_vars]
    g = work.groupby(group_cols, dropna=False)["_Ynum"]

    means = g.mean()
    # ddof=0 -> שונות 'אוכלוסייה' עבור plug-in; אפשר לשנות ל-ddof=1 אם תרצי שונות מדגמית.
    vars_ = g.var(ddof=0).fillna(0.0)

    b_map: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}
    var_map: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}

    for idx, m in means.items():
        a_val = idx[0]
        z_tuple = tuple(idx[1:])
        b_map[(a_val, z_tuple)] = float(m)
        var_map[(a_val, z_tuple)] = float(max(vars_.get(idx, 0.0), 0.0))

    return b_map, var_map


# -----------------------------------------
# Step 2: P(Z) and P(A,Z) from data
# -----------------------------------------

def estimate_PZ_and_PAZ(
    df: pd.DataFrame,
    *,
    A: str,
    Z_vars: Sequence[str],
    alpha: float = 0.0,
    A_domain: Optional[Sequence[Any]] = None,
) -> Tuple[Dict[Tuple[Any, ...], float], Dict[Tuple[Any, Tuple[Any, ...]], float]]:
    """
    מאמד מהנתונים:
      P(Z=z) ו- P(A=a, Z=z)

    alpha:
      Laplace smoothing:
      - alpha=0  => אמדן תדירויות רגיל.
      - alpha>0  => מוסיפים alpha פסאודו-ספירות לכל תא (a,z).
        זה עוזר למנוע f(a|z)=0 אך משנה את האמידה במדגם קטן.

    A_domain:
      אם ידוע מראש (למשל bn.domains[A]) עדיף להעביר כדי שהחלקה תהיה עקבית.
    """
    _validate_columns(df, [A, *Z_vars])
    n = len(df)
    if n == 0:
        raise ValueError("Empty observations dataframe.")

    # Domains (for smoothing)
    if A_domain is None:
        A_vals = sorted(df[A].dropna().unique().tolist())
    else:
        A_vals = list(A_domain)

    # count observed Z
    z_counts = df.groupby(list(Z_vars), dropna=False).size()
    # normalize keys to tuples
    observed_z = []
    for z_idx in z_counts.index:
        z_tuple = _as_tuple(z_idx, len(Z_vars))
        observed_z.append(z_tuple)

    # PZ smoothing: easiest is to smooth only through induced PAZ smoothing.
    # We'll compute PAZ with smoothing, then derive PZ by summing over A.
    # Build raw counts for (A,Z)
    az_counts = df.groupby([A, *Z_vars], dropna=False).size()

    # Build PAZ with optional smoothing over *observed z* (and full A domain)
    # If you want smoothing over unobserved z combinations too, you'd need full Cartesian product of Z domains,
    # which can explode; we avoid that for efficiency.
    PAZ_counts: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}
    total_mass = 0.0

    for z_tuple in observed_z:
        for a_val in A_vals:
            idx = (a_val, *z_tuple)
            c = float(az_counts.get(idx, 0.0))
            c_smooth = c + float(alpha)
            PAZ_counts[(a_val, z_tuple)] = c_smooth
            total_mass += c_smooth

    # Convert counts to probabilities
    PAZ: Dict[Tuple[Any, Tuple[Any, ...]], float] = {
        k: v / total_mass for k, v in PAZ_counts.items()
    }

    # PZ is sum over A
    PZ_counts: Dict[Tuple[Any, ...], float] = {z: 0.0 for z in observed_z}
    for (a_val, z_tuple), p in PAZ.items():
        PZ_counts[z_tuple] += p

    PZ = dict(PZ_counts)
    return PZ, PAZ


# -----------------------------------------
# Step 3: f(A|Z) from PZ and PAZ
# -----------------------------------------

def compute_f_A_given_Z(
    PZ: Mapping[Tuple[Any, ...], float],
    PAZ: Mapping[Tuple[Any, Tuple[Any, ...]], float],
) -> Dict[Tuple[Any, Tuple[Any, ...]], float]:
    """
    מחשב:
      f(a|z) = P(A=a, Z=z) / P(Z=z)
    """
    f_map: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}
    for (a_val, z_tuple), p_az in PAZ.items():
        pz = float(PZ.get(z_tuple, 0.0))
        f_map[(a_val, z_tuple)] = 0.0 if pz <= 0.0 else float(p_az) / pz
    return f_map


# -----------------------------------------
# Step 4: π(A|L) over observed Z
# -----------------------------------------

def compute_pi_map(
    *,
    df: pd.DataFrame,
    A: str,
    Z_vars: Sequence[str],
    L_vars: Sequence[str],
    policy_fn: PolicyFn,
    A_domain: Sequence[Any],
) -> Dict[Tuple[Any, Tuple[Any, ...]], float]:
    """
    מחשב π(a|L(z)) לכל (a,z) עבור z שנצפו בדאטה.

    יעילות:
      - מחשבים רק עבור קומבינציות L שנצפו.
      - ואז מרחיבים לכל z (כי π תלויה רק ב-L, לא בכל Z).
    """
    _validate_columns(df, [A, *Z_vars, *L_vars])

    # map from L_tuple -> {a: pi}
    L_unique = df[list(L_vars)].drop_duplicates()
    pi_by_L: Dict[Tuple[Any, ...], Dict[Any, float]] = {}

    for _, lr in L_unique.iterrows():
        L_tuple = tuple(lr[l] for l in L_vars)
        L_assign = {l: lr[l] for l in L_vars}
        pi_by_L[L_tuple] = {a: float(policy_fn(a, L_assign)) for a in A_domain}

    # now for each observed Z, attach the corresponding L_tuple
    z_unique = df[list(Z_vars)].drop_duplicates()
    pi_map: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}

    for _, zr in z_unique.iterrows():
        z_tuple = tuple(zr[z] for z in Z_vars)
        L_tuple = tuple(zr[l] for l in L_vars)
        pis = pi_by_L[L_tuple]
        for a in A_domain:
            pi_map[(a, z_tuple)] = float(pis[a])

    return pi_map


# -----------------------------------------
# Step 5: weights w = π / f
# -----------------------------------------

def compute_weights_w(
    pi_map: Mapping[Tuple[Any, Tuple[Any, ...]], float],
    f_map: Mapping[Tuple[Any, Tuple[Any, ...]], float],
    *,
    clip_max: Optional[float] = None,
) -> Dict[Tuple[Any, Tuple[Any, ...]], float]:
    """
    מחשב:
      w(a,z) = π(a|L(z)) / f(a|z)

    clip_max:
      אופציונלי: קיטום משקולות למניעת התפוצצות במדגם קטן (לא חלק מהמאמר, אבל שימושי בפועל).
    """
    w_map: Dict[Tuple[Any, Tuple[Any, ...]], float] = {}
    for key, pi_val in pi_map.items():
        f_val = float(f_map.get(key, 0.0))
        w = 0.0 if f_val <= 0.0 else float(pi_val) / f_val
        if clip_max is not None:
            w = float(np.clip(w, 0.0, clip_max))
        w_map[key] = w
    return w_map


# -----------------------------------------
# Step 6: m(z) = Σ_a b(a,z) π(a|L(z))
# -----------------------------------------

def compute_m_Z(
    *,
    Z_vars: Sequence[str],
    L_vars: Sequence[str],
    A_domain: Sequence[Any],
    b_map: Mapping[Tuple[Any, Tuple[Any, ...]], float],
    policy_fn: PolicyFn,
    observed_z_tuples: Iterable[Tuple[Any, ...]],
) -> Dict[Tuple[Any, ...], float]:
    """
    מחשב:
      m(z) = Σ_a b(a,z) * π(a|L(z))

    הערה:
      כאן אנחנו משתמשים ישירות ב-policy_fn כדי לא להסתמך על pi_map,
      וכדי לאפשר מקרה שבו b_map קיים אבל לא חישבנו pi_map מראש.
    """
    m_map: Dict[Tuple[Any, ...], float] = {}

    for z_tuple in observed_z_tuples:
        # Build L_assign from z_tuple
        z_dict = {Z_vars[i]: z_tuple[i] for i in range(len(Z_vars))}
        L_assign = {l: z_dict[l] for l in L_vars}

        m = 0.0
        for a in A_domain:
            b = float(b_map.get((a, z_tuple), 0.0))
            pi = float(policy_fn(a, L_assign))
            m += b * pi

        m_map[z_tuple] = float(m)

    return m_map


# -----------------------------------------
# Step 7: χ = E[m(Z)] = Σ_z m(z) P(Z=z)
# -----------------------------------------

def compute_chi_pi_Z(
    PZ: Mapping[Tuple[Any, ...], float],
    m_map: Mapping[Tuple[Any, ...], float],
) -> float:
    chi = 0.0
    for z_tuple, pz in PZ.items():
        chi += float(pz) * float(m_map.get(z_tuple, 0.0))
    return float(chi)


# -----------------------------------------
# Step 8: ψ_i for each observation
# -----------------------------------------

def compute_influence_values(
    df: pd.DataFrame,
    *,
    Y: str,
    A: str,
    Z_vars: Sequence[str],
    L_vars: Sequence[str],
    bn_domains_A: Sequence[Any],
    policy_fn: PolicyFn,
    nuisance: NuisanceEstimates,
    value_map: Optional[Mapping[Any, float]] = None,
) -> np.ndarray:
    """
    מחשב ψ_i לכל תצפית i (ערך פונקציית ההשפעה).

    ψ(O) = π(A|L)/f(A|Z) * (Y - b(A,Z)) + m(Z) - χ

    הערות:
      - b_map/f_map/m_map/chi נלקחים מתוך nuisance שכבר חושבו.
      - אם תא (a,z) לא קיים במפות (מדגם קטן), אנו משתמשים ב-0.0 כברירת מחדל.
        אם את מעדיפה לזרוק שגיאה במצב כזה, אפשר לשנות.
    """
    _validate_columns(df, [Y, A, *Z_vars, *L_vars])

    # map Y to numeric
    if value_map is None:
        uniq = set(df[Y].dropna().unique().tolist())
        if uniq.issubset({0, 1}):
            value_map = {0: 0.0, 1: 1.0}
        else:
            raise ValueError(
                "Y אינו בינארי {0,1}. ספקי value_map שממפה כל ערך אפשרי של Y למספר."
            )

    y_num = df[Y].map(value_map).astype(float).to_numpy()

    # Build arrays for b, f, m, pi
    b_arr = np.zeros(len(df), dtype=float)
    f_arr = np.zeros(len(df), dtype=float)
    m_arr = np.zeros(len(df), dtype=float)
    pi_arr = np.zeros(len(df), dtype=float)

    chi = float(nuisance.chi)

    # Iterate rows (fast enough for moderate n; for very large n can vectorize via merge)
    for i, row in enumerate(df.itertuples(index=False)):
        row_dict = row._asdict()
        a_val = row_dict[A]
        z_tuple = tuple(row_dict[z] for z in Z_vars)

        b_arr[i] = float(nuisance.b_map.get((a_val, z_tuple), 0.0))
        f_arr[i] = float(nuisance.f_map.get((a_val, z_tuple), 0.0))
        m_arr[i] = float(nuisance.m_map.get(z_tuple, 0.0))

        # π(a|L) computed by policy_fn (robust to not having pi_map key)
        L_assign = {l: row_dict[l] for l in L_vars}
        pi_arr[i] = float(policy_fn(a_val, L_assign))

    # weight term: pi / f, guard f=0
    with np.errstate(divide="ignore", invalid="ignore"):
        w = np.where(f_arr > 0.0, pi_arr / f_arr, 0.0)

    psi = w * (y_num - b_arr) + m_arr - chi
    return psi


# -----------------------------------------
# Step 9: σ^2 estimator
# -----------------------------------------

def estimate_sigma2_from_psi(psi: Union[np.ndarray, Sequence[float]]) -> float:
    """
    אומדן השונות האסימפטוטית:
      σ^2 = E[ψ^2]  ->  \hat σ^2 = mean(ψ_i^2)
    """
    psi_arr = np.asarray(psi, dtype=float)
    return float(np.mean(psi_arr ** 2))


def estimate_asymptotic_variance_pi_Z(
    df: pd.DataFrame,
    *,
    bn,  # bn instance (for domains)
    Y: str,
    A: str,
    Z_vars: Sequence[str],
    L_vars: Sequence[str],
    policy_fn: PolicyFn,
    value_map: Optional[Mapping[Any, float]] = None,
    alpha: float = 0.0,
    clip_w_max: Optional[float] = None,
) -> Tuple[float, NuisanceEstimates]:
    """
    פונקציית-על שמבצעת את כל השלבים ומחזירה:
      - sigma2_hat : אומדן השונות האסימפטוטית
      - nuisance   : כל האמידות הביניים (ל-debug/אנליזה)

    alpha:
      Laplace smoothing ל-P(A,Z) (ומכאן ל-f). alpha=0 => ללא החלקה.

    clip_w_max:
      קיטום משקולות אופציונלי (לא חלק מהמאמר; עוזר להימנע מהתפוצצות במדגם קטן).
    """
    _validate_columns(df, [Y, A, *Z_vars, *L_vars])

    A_domain = list(bn.domains[A])

    # 1) b(a,z), Var(Y|a,z)
    b_map, var_map = estimate_b_and_var_Y_given_AZ(
        df, Y=Y, A=A, Z_vars=Z_vars, value_map=value_map
    )

    # 2) PZ, PAZ (עם/בלי smoothing)
    PZ, PAZ = estimate_PZ_and_PAZ(
        df, A=A, Z_vars=Z_vars, alpha=alpha, A_domain=A_domain
    )

    # 3) f(A|Z)
    f_map = compute_f_A_given_Z(PZ, PAZ)

    # 4) π(a|L(z)) map (לכל (a,z) שנצפה)
    pi_map = compute_pi_map(
        df=df, A=A, Z_vars=Z_vars, L_vars=L_vars, policy_fn=policy_fn, A_domain=A_domain
    )

    # 5) w(a,z)
    w_map = compute_weights_w(pi_map, f_map, clip_max=clip_w_max)

    # observed z tuples from PZ keys
    observed_z = list(PZ.keys())

    # 6) m(z)
    m_map = compute_m_Z(
        Z_vars=Z_vars,
        L_vars=L_vars,
        A_domain=A_domain,
        b_map=b_map,
        policy_fn=policy_fn,
        observed_z_tuples=observed_z,
    )

    # 7) chi
    chi = compute_chi_pi_Z(PZ, m_map)

    nuisance = NuisanceEstimates(
        b_map=b_map,
        var_map=var_map,
        PZ=dict(PZ),
        PAZ=dict(PAZ),
        f_map=f_map,
        pi_map=pi_map,
        w_map=w_map,
        m_map=m_map,
        chi=chi,
    )

    # 8) psi_i
    psi = compute_influence_values(
        df,
        Y=Y,
        A=A,
        Z_vars=Z_vars,
        L_vars=L_vars,
        bn_domains_A=A_domain,
        policy_fn=policy_fn,
        nuisance=nuisance,
        value_map=value_map,
    )

    # 9) sigma^2 = mean(psi^2)
    sigma2_hat = estimate_sigma2_from_psi(psi)
    return sigma2_hat, nuisance


# -----------------------------------------
# Convenience: compute over multiple Z sets
# -----------------------------------------

def estimate_asymptotic_variance_over_Z_sets(
    df: pd.DataFrame,
    *,
    bn,
    Y: str,
    A: str,
    Z_sets: Sequence[Sequence[str]],
    L_vars: Sequence[str],
    policy_fn: PolicyFn,
    value_map: Optional[Mapping[Any, float]] = None,
    alpha: float = 0.0,
    clip_w_max: Optional[float] = None,
) -> Dict[Tuple[str, ...], float]:
    """
    מחשב אומדן σ^2 עבור כל קבוצת התאמה Z ב-Z_sets.

    מחזיר:
      results[tuple(sorted(Z))] = sigma2_hat
    """
    results: Dict[Tuple[str, ...], float] = {}
    for Z in Z_sets:
        Z_key = tuple(sorted(Z))
        sigma2_hat, _ = estimate_asymptotic_variance_pi_Z(
            df,
            bn=bn,
            Y=Y,
            A=A,
            Z_vars=list(Z),
            L_vars=list(L_vars),
            policy_fn=policy_fn,
            value_map=value_map,
            alpha=alpha,
            clip_w_max=clip_w_max,
        )
        results[Z_key] = float(sigma2_hat)
    return results
